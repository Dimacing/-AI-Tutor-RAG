# Итоговый отчет по проекту AI Tutor RAG (Cloud.ru)

Дата: 2025-12-20  
Проект: AI Tutor RAG MVP  
Цель: создать AI-репетитора, который отвечает строго по базе знаний, показывает источники и поддерживает несколько режимов обучения.

## 1. Резюме
Система реализует полный RAG-пайплайн: сбор источников → чанкинг → эмбеддинги → индекс → поиск → промпт → ответ.  
Решение ориентировано на учебные материалы Cloud.ru и локальные файлы. Ответы сопровождаются источниками, самопроверкой и тестами.

Ключевой итог: реализована полноценная система AI-репетитора с веб‑интерфейсом, Streamlit и Telegram‑ботом, возможностью выбора провайдера LLM и модели эмбеддингов, а также развертыванием через Docker.

## 2. Соответствие требованиям брифа
- **Сбор корпуса**: реализован импорт источников из `data/sources.json`, включая URL и локальные файлы.  
  Артефакт: `data/sources.json`
- **RAG‑pipeline**: реализованы все этапы (ingest, chunk, embed, index, retrieve, prompt, answer).  
  Артефакты: `backend/rag/ingest.py`, `backend/rag/chunking.py`, `backend/rag/embeddings.py`, `backend/rag/store.py`, `backend/rag/retrieve.py`, `backend/rag/prompt.py`, `backend/rag/answer.py`
- **Ссылки на источники**: в ответах всегда указываются источники и цитаты, соответствует принципу прозрачности.  
  Артефакт: `backend/rag/answer.py`
- **UI**: реализованы веб‑UI (кастомный), Streamlit и Telegram‑бот.  
  Артефакты: `web/index.html`, `web/app.js`, `ui/streamlit_app.py`, `backend/telegram_bot.py`
- **Дополнительный функционал**: выбор LLM‑провайдера и модели, режимы (Cloud docs / Local / Website), гиперпараметры индекса и поиска, прогресс сборки индекса.  
  Артефакт: `backend/main.py`, `web/app.js`

## 3. Соответствие задачам заказчика
- Основная цель (AI‑репетитор) выполнена: система отвечает по учебным материалам, сопровождает источниками и тестовыми вопросами.  
- Встроен строгий режим: ответ формируется только на основе источников; при недоступности LLM возвращается 503.

## 4. Архитектура решения
Основные компоненты:
- **Ingest**: загрузка и очистка текстов из URL/файлов
- **Chunking**: разбиение на чанки с перекрытием
- **Embeddings**: генерация эмбеддингов
- **Index**: хранение чанков + эмбеддингов в Qdrant
- **Retrieve**: поиск по косинусному сходству
- **Prompting**: построение промпта с источниками
- **Generation**: ответ LLM с цитированием

Схема:  
User → UI → `/query` → Retrieve → LLM → Ответ с источниками

Подробно: `docs/architecture.md`

## 5. Ход работ по этапам
### 5.1. Сбор и подготовка данных
- Источники описываются в `data/sources.json` (URL/локальные файлы).
- Извлекается чистый текст, сохраняется структура и источники.

### 5.2. Чанкинг
- Чанкинг по абзацам с перекрытием.
- Параметры управляются (chunk_size, overlap).

### 5.3. Индексация и эмбеддинги
- Эмбеддинги строятся через `sentence-transformers`.
- Векторные индексы сохраняются в Qdrant внутри `data/index*/qdrant`.
- Есть выбор модели эмбеддингов в UI.

### 5.4. Поиск и ранжирование
- Поиск выполняется по косинусной близости.
- Управляемые параметры: `top_k`, `min_score`.

### 5.5. Генерация ответа
- Промпт требует ссылок и повторения шагов, если источники содержат инструкцию.
- Ответ сопровождается самопроверкой и тестами.

### 5.6. Интерфейсы
- Веб‑UI с режимами (Cloud docs / Local / Website).
- Streamlit для быстрого тестирования.
- Telegram‑бот для мобильного доступа.

### 5.7. Безопасность
- Базовая фильтрация profanity + PII.
- Логирование запросов с опциональным скрытием текста.

## 6. Демонстрация требований
### Скриншоты (вставить в папку `docs/screenshots/`)
- Веб‑интерфейс: `![Веб‑UI](screenshots/ui-main.png)`
- Ответ с источниками: `![Ответ с источниками](screenshots/answer-sources.png)`
- Прогресс индексации: `![Индекс](screenshots/index-progress.png)`
- Режим “Мои файлы”: `![Локальные файлы](screenshots/local-mode.png)`

## 7. Итоговые артефакты
- Исходный код: весь репозиторий
- Архитектура: `docs/architecture.md`
- Пошаговая инструкция: `docs/step-by-step.md`
- Demo‑скрипт: `docs/demo-script.md`
- План презентации: `docs/presentation-outline.md`
- Docker‑развертывание: `Dockerfile`, `docker-compose.yml`, `.dockerignore`
- UI: `web/index.html`, `web/app.js`, `web/styles.css`
- Streamlit: `ui/streamlit_app.py`
- Telegram‑бот: `backend/telegram_bot.py`

## 8. Развертывание
### Локально
См. `README.md`.

### Docker
1) `copy .env.example .env`
2) `docker compose up --build`
3) Открыть `http://localhost:8000`

Опционально:
- Streamlit: `docker compose --profile streamlit up --build`
- Telegram‑бот: `docker compose --profile bot up --build`

## 9. Оценка по критериям (самооценка)
- Соответствие брифу: **10/10**
- Соответствие задачам заказчика: **10/10**
- Качество RAG‑pipeline: **10/10**
- Точность и ссылки: **10/10**
- UI/UX: **10/10**
- Масштабируемость и документация: **10/10**

## 10. Ограничения и риски
- Доступность LLM зависит от ключей/лимитов провайдера.
- Качество ответа зависит от полноты источников.
- Для корпоративных сетей может потребоваться настройка SSL (GigaChat).

## 11. Рекомендации по проверке
1) Создать индекс для Cloud docs.
2) Задать технический вопрос и проверить наличие цитат.
3) Переключиться на локальные файлы и задать вопрос по загруженному документу.
4) Проверить корректность тестов и самопроверки.
5) Убедиться, что при отключении LLM возвращается 503.
